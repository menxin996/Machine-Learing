# 习题五

**习题5-1**

**1)证明公式(5.6)可以近似为离散信号序列x(t)关于t的二阶微分;2)对于二维卷积，设计一种滤波器来近似实现对二维输入信号的二阶微分。**

(1)

证：$x''(t)=x(t+1)+x(t-1)-2x(t)$

首先从一维函数x(t)来推导：

$\frac{∂x}{∂t}=lim_{\Delta t->0}\frac{x(t+\Delta t)-x(t)}{\Delta t}$

所以二阶微分为：

$\frac{∂^2x}{∂^2t}=lim_{\Delta t->0}\frac{x'(t+\Delta t)-x'(t)}{\Delta t}$

因为信号是根据时间来离散的，最小的Δt是1个时间单位，因此：

$\frac{∂x}{∂t}=x'(t)=x(t+1)-x(t)$

那么二阶微分为：

$\frac{∂^2x}{∂^2t}=\frac{∂x'(t)}{∂t^2}=x'(t+1)-x'(t)\\=x(t+2)-x(t+1)-x(t+1)+x(t)\\
=x(t+2)-2x(t+1)+x(t)$

令x=x-1，得:

$x''(t)=x(t+1)+x(t-1)-2x(t)$

(2)

解：

设二维输入信号是f(x,y)，在x和y两个方向上，用到(1)的结论：

$\frac{∂^2f}{∂x^2}=f(x+1,y)+f(x-1,y)-2f(x,y)\\
\frac{∂^2f}{∂y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)$

将x方向和y方向的二阶导数结合在一起：

$\frac{∂^2f}{∂x^2}+\frac{∂^2f}{∂y^2}=f(x+1,y)+f(x-1,y)+f(x,y+1)\\+f(x,y-1)-4f(x,y)$

所以滤波器可以设置为：(不是很确定😵)

$\omega=\left[
\matrix{
  1 & -2 & 1\\
  1 & -2 & 1\\
  1 & -2 & 1 
}
\right]$

<hr>

**习题5-2**

**证明宽卷积具有交换性，即公式(5.13)**

证：

$rot180(W)õX=rot180(X)õW$

一脸懵逼，不知道怎么下手。

证明一下卷积的交换性吧。

$f(t)*g(t)=\int_{-∞}^{+∞}{f(\tau)*g(t-\tau)}d\tau$

令t-τ=λ，则：

$\tau:\int_{-∞}^{+∞}->\lambda:\int_{+∞}^{-∞}\\
d\tau=-d\lambda$

负负得正，因此积分上下限不变，即：

$f(t)*g(t)=\int_{-∞}^{+∞}{g(\lambda)*f(t-\lambda)}d\lambda=g(t)*f(t)$

<hr>
**习题5-3**

**分析卷积神经网络中用1×1的卷积核的作用**

答：(1)特征降维，节省计算量。

​		(2)增加模型的非线性表达能力。

Eg:

**以GoogLeNet的3a模块为例**

输入：28\*28\*192

1×1的卷积通道：64

3×3的卷积通道：128

5×5的卷积通道：32

**(1)不加1×1卷积层，卷积核参数为：**

192 × (1×1×64) +192 × (3×3×128) + 192 × (5×5×32) = 387072

**(2)在3×3和5×5卷积层前加入通道数为96和16的1×1卷积层，卷积核参数为：**

1×1×192×64+（1×1×192×96+3×3×96×128）+（1×1×192×16+5×5×16×32）= 157184

**参数大约减少到原来的1/3**

<hr>

**习题5-4**

**对于一个输入为100×100×64的特征映射组，使用3×3的卷积核，输出为100×100×256的特征映射组的卷积层，求其时间和空间复杂度。如果引入一个1×1卷积核，先得到100×100×64的特征映射，再进行3×3的卷积，得到100×100×256的特征映射组，求其时间和空间复杂度。**

解：

$Time～O(M^2*K^2*C_{in}*C_{out})\\
Space～O(\sum_{l=1}^{D}{K_l^2*C_{l-1}*C_{l}})$

(1)

时间和空间复杂度：

$Time～O(100^2*3^2*256*256)=O(5.89824×10^9)\\
Space～O(3^2*256*256)=O(5.89824×10^5)$

(2)

时间和空间复杂度：

$Time～O(100^2*1^2*256*64)+O(100^2*3^2*256*64)\\=O(1.6384×10^9)\\
Space～O(1^2*256*64+3^2*256*64)\\=O(1.6384×10^5)$

<hr>

**习题5-5**

**对于一个二维卷积，输入为3×3，卷积核大小为2×2，试将卷积操作重写为仿射变换的形式。**

解：

仿射变换是用来实现高维到低维的映射，形式为：

$z=Wx$

由题意得，3维向量x，经过2维的卷积核ω=[ω<sub>1</sub>,ω<sub>2</sub>]<sup>T</sup>进行卷积，仿射变换形式如下所示：

$z=\left[
\matrix{
  \omega1 & \omega2 & 0\\
  0 & \omega1 & \omega2\\
}
\right]x$

<hr>

**习题5-6**

**计算函数y=max(x<sub>1</sub>,...,x<sub>D</sub>)和函数y=argmax(x<sub>1</sub>,...,x<sub>D</sub>)的梯度**

解：

首先需要弄明白max函数和argmax函数的区别：

<ul>
    <li>y=max f(x)：y是f(x)函数的最大值</li>
    <li>y=argmax f(x)：y是f(x)取到最大值时的参数x</li>
</ul>

**接着判断y=max(x<sub>1</sub>,...,x<sub>D</sub>)**，因为是取x<sub>1</sub>,...,x<sub>D</sub>中最大的一个，比如说x<sub>1</sub>最大，则y对x的梯度为[1,0,...,0]，即**只有一个为1，其余全为0的D维度行向量**。

**最后判断y=argmax(x<sub>1</sub>,...,x<sub>D</sub>)**，取x<sub>1</sub>,...,x<sub>D</sub>中最大的一个的下标，比如x<sub>1</sub>最大,则y=0(下标从0~D-1)，因此y对x的梯度是[0,...,0]，因为相当于是常数C对变量求导，即**全为0的D维行向量。**

<hr>
**习题5-7**

**忽略激活函数，分析卷积网络中卷积层的前向计算和反向传播(公式(5.39))是一种转置关系。**

分析：

目前不会证明，希望有会证明的小伙伴告知一下。

<hr>

**习题5-8**

**在空洞卷积中，当卷积核大小为K，膨胀率为D时，如何设置零填充P的值使得卷积成为等宽卷积。**

等宽卷积指：输入神经元个数和输出长度一致。

假设输入神经元个数为M，卷积大小为K，步长为S，在输入两端补上P个0，那么卷积层神经元数量为(M-K+2P)/S+1。

由题意得，膨胀率为D，因此K'=K+(K-1)×(D-1)

解方程M-K'+2P+1=M，得：

2P+1=K+(K-1)×(D-1)

P=$\frac{K+(K-1)×(D-1)-1}{2}$