# 习题三

**习题3-1**

​	**证明在两类线性分类中，权重向量ω和决策平面正交。**

证：设x<sub>1</sub>和x<sub>2</sub>是决策平面上的两个向量，二类线性分类的决策平面需要满足：

$\omega x_1+b=0\\
\omega x_2+b=0$

因此：

$\omega(x_1-x_2)=0\\$

而设x<sub>1</sub>-x<sub>2</sub>是决策平面上不同于x<sub>1</sub>和x<sub>2</sub>的向量。

因此，在二类线性分类中，权重向量ω和决策平面正交。

<hr>

**习题3-2**

​	**在线性空间中，证明一个点x到平面f(x;ω)=ω<sup>T</sup>x+b=0的距离为|f(x;ω)|/||ω||**。

证：

$d=\frac{|\omega*P_1P_2|}{||\omega||}$

可以类比二维平面，设平面上的一点P<sub>1</sub>(x<sub>1</sub>,y<sub>1</sub>)和原点组成的向量为x<sub>1</sub>，平面外的一点P<sub>2</sub>(x<sub>2</sub>,y<sub>2</sub>)和原点组成的向量x<sub>2</sub>，P<sub>1</sub>P<sub>2</sub>=x<sub>2</sub>-x<sub>1</sub>，则：

$d=\frac{|\omega*(x_2-x_1)|}{||\omega||}\\
\omega x_1+b=0$

原式=

$d=\frac{|\omega*x_2+b|}{||\omega||}$

因此，存在一个点x到平面的距离为|ω<sup>T</sup>x+b|/||ω||

<hr>
**习题3-3**

​	**在线性分类中，决策区域是凸的。即若点x<sub>1</sub>和x<sub>2</sub>被分类成类别c，则点ρx<sub>1</sub>+(1-ρ)x<sub>2</sub>也会被分为类别c，其中ρ∈(0,1)**

运用**多分类方式中的argmax方式**进行证明

$\because x_1,x_2被分类成类别c\\
\therefore\omega_cx_1>\omega_{\overline c}x_1 \quad \omega_cx_2>\omega_{\overline c}x_2\\
\therefore \rho \omega_cx_1>\rho\omega_{\overline c}x_1 \quad (1-\rho) \omega_cx_2>(1-\rho)\omega_{\overline c}x_2\\
\therefore \omega_c(\rho x_1+(1-\rho) x_2)>\omega_{\overline c}(\rho x_1+(1-\rho)x_2)\\
\therefore \rho x_1+(1-\rho)x_2被分类为类别c$

<hr>

**习题3-4**

​	**给定一个多分类的数据集，证明：1)如果数据集中的每个类的样本都和除该类之外的样本是线性可分的，则该数据集一定是线性可分的。2)如果数据集中每两个类的样本是线性可分的，则该数据集不一定是线性可分的。**

(1)证：因为每个类的样本和除该类之外的样本是线性可分的，所以每个类都可以和其他类单独区分开来，因此，整个数据集一定是线性可分的。

(2)证：每两个类的样本是线性可分的可能会出现冲突，存在一些样本难以确定类别。

举个例子：预测的类别有1、2、3三种，因为每两个类的样本是线性可分的，因此，f13将样本分成1、3两类，f12将样本分成1、2两类，f23将样本分成2、3两类。可能会出现公共区域，这个区域无法区分样本是属于哪一类的。如下图?区域所示:

![12](C:\Users\lenovo\Desktop\study\picture\3-4.png)

<hr>

**习题3-5**


​	**在Logistic回归中，是否可以用**

$\widehat{y}=\sigma(w^Tx)$

**去逼近正确的标签y，并用平方损失(y-y^hat)²最小化来优化参数ω？**

答：不可以。Logistic回归是一种常用的处理二分类问题的线性模型，是用来预测类别标签的后验概率，连续性函数

$\widehat{y}=\sigma(w^Tx)$

不适合进行分类问题，需要非线性函数将线性函数压缩到(0,1)之间。平方损失函数不适合分类问题，在分类问题中，每个标签的距离是没有意义的，因此平方损失求得的预测值和真实标签的平方差不能反映这个问题的优化程度。

<hr>

**习题3-6**

​	**在Softmax回归的风险函数(公式(3.39))中，如果加上正则化项会有什么影响?**

答：正则化就是对最小化经验误差函数加上约束。约束有引导作用，倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识。正则化的作用是选择经验风险函数与模型复杂度同时较小的模型，能减少噪声的影响，降低过拟合。

<hr>

**习题3-7**

​	**验证平均感知器训练算法3.2中给出的平均权重向量的计算方式和公式(3.77)等价。**

<hr>

**习题3-8**


​	**证明定理3.2：广义感知器参数学习算法的权重更新次数不超过R^2/γ^2**

证：

广义感知器中的权重向量的更新方式：

$\omega_{k+1}=\omega_k+(\phi(x^{n},y^{n})-\phi(x^{n}-\widehat{y}^{n}))$

其中，x<sup>(n)</sup>，y<sup>(n)</sup>表示第n个错误分类的样本，因为初始权重向量为0，在第N次更新时感知器的权重向量为：

$\omega_N=\sum_{n=1}^{N}{(\phi(x^{n},y^{n})-\phi(x^{n}-\widehat{y}^{n}))}$

分别计算||ω<sub>N</sub>||²的上下界：

(1)||ω<sub>N</sub>||²的上界

$||\omega_N||^2=||\omega_{N-1}+(\phi(x^n,y^n)-\phi(x^n,\widehat{y}^n))||^2\\
=||\omega_{N-1}||^2+(\phi(x^n,y^n)-\phi(x^n,\widehat{y}^n))^2+\\2\omega_{N-1}^T(\phi(x^n,y^n)-\phi(x^n,\widehat{y}^n))\\
\because 2\omega_{N-1}^T(\phi(x^n,y^n)-\phi(x^n,\widehat{y}^n))≤0\\
\therefore 原式≤||\omega_{N-1}||^2+R^2
≤||\omega_{N-2}||^2+2R^2\\
≤NR^2$

(2)||ω<sub>N</sub>||²的下界

$||\omega_N||^2=||\omega^*||^2·||\omega_N||^2≥||{\omega^*}^T\omega_N||^2\\
=||{\omega^*}^T\sum_{n=1}^{N}{(\phi(x^{n},y^{n})-\phi(x^{n}-\widehat{y}^{n}))}||\\≥(N\gamma)^2=N^2\gamma^2$

因此：

$N^2\gamma^2≤||\omega_N||^2≤NR^2\\
\therefore N≤\frac{R^2}{\gamma^2}$

<hr>

**习题3-9**


​	**若数据集线性可分，证明支持向量机中将两类样本正确分开的最大间隔分割超平面存在且唯一。**

证：最大间隔分割超平面

$min\quad\frac{1}{2}||\omega||^2\\
s.t.\quad y_i(\omega·x_i+b)-1≥0,i=1,2,3,...,N$

(1)证明存在性

由于目标函数有下界，所以最优化问题必有最优解，记做(ω<sup>\*</sup>, b<sup>*</sup>)

因为数据集中有正类点和负类点，当哦么ω为0时，不存在超平面将正类点和负类点分开，因此ω=0不是可行解，所以最优解中的ω<sup>\*</sup>一定不能等于0，所以超平面存在。

(2)证明唯一性

反证法：假设(ω<sub>1</sub><sup>\*</sup>,b<sub>1</sub><sup>\*</sup>)，(ω<sub>2</sub><sup>\*</sup>,b<sub>2</sub><sup>\*</sup>)都是最优解，即||ω<sub>1</sub><sup>\*</sup>||和||ω<sub>2</sub><sup>\*</sup>||都为最小值c

$y_i(\omega_1^*·x_i+b_1^*)-1≥0\\
y_i(\omega_2^*·x_i+b_2^*)-1≥0$

两式相加得：

$y_i(\frac{\omega_1^*+\omega_2^*}{2}·x_i+\frac{b_1^*+b_2^*}{2})-1≥0$

令

$\omega=\frac{\omega_1^*+\omega_2^*}{2} \quad b=\frac{b_1^*+b_2^*}{2}$

可得(ω，b)为最优化问题的可行解。

$\because||a+b||≤||a||+||b||\\
\therefore c≤||\omega||≤\frac{||\omega_1^*||}{2}+\frac{||\omega_2^*||}{2}=c\\
\therefore \omega_1^*=\lambda\omega_2^* \quad \lambda=±1\\
\because\lambda=-1时,\omega=0不是可行解\\
\therefore \lambda=1 \quad \omega_1^*=\omega_2^*$

所以ω唯一。此时最优解变成(ω<sup>*</sup>,b<sub>1</sub><sup>\*</sup>)，(ω<sup>\*</sup>,b<sub>2</sub><sup>\*</sup>)。

在超平面(ω<sup>*</sup>,b<sub>1</sub><sup>\*</sup>)中找到x<sub>1</sub>和x<sub>1</sub><sup>\'</sup>使得

$\omega^*x_1+b_1^*-1=0\\
-\omega^*x_1^{'}-b_1^*-1=0$

在超平面(ω<sup>*</sup>,b<sub>2</sub><sup>\*</sup>)中找到x<sub>2</sub>和x<sub>2</sub><sup>\'</sup>使得

$\omega^*x_2+b_2^*-1=0\\
-\omega^*x_2^{'}-b_2^*-1=0$

联立上述四个等式可得：

$b_1^*=-\frac{1}{2}(\omega^*x_1+\omega^*x_1^{'})\\
b_2^*=-\frac{1}{2}(\omega^*x_2+\omega^*x_2^{'})\\
b_1^*-b_2^*=-\frac{1}{2}(\omega^*(x_1-x_2)+\omega^*(x_1^{'}-x_2^{'}))$

将x<sub>1</sub>代入超平面(ω<sup>\*</sup>,b<sub>2</sub><sup>\*</sup>)，x<sub>2</sub>代入超平面(ω<sup>*</sup>,b<sub>1</sub><sup>\*</sup>)得：

$\omega^*x_1+b_2^*≥1=\omega^*x_2+b_2^*\\
\omega^*x_2+b_1^*≥1=\omega^*x_1+b_1^*$

因此:

$\omega^*(x_1-x_2)≥0\\
\omega^*(x_2-x_1)≥0\\
\therefore \omega^*(x_2-x_1)=0$

同理可得:

$\omega^*(x_2^{'}-x_1^{'})=0$

因此:

$b_1^*=b_2^*$

所以，b唯一，超平面唯一。

综上所述，超平面存在且唯一。

<hr>

**习题3-10**

​	**验证公式(3.97)**

$k(x,z)=(1+x^Tz)^2=\phi(x)^T\phi(z)$

证：

$k(x,z)=(1+x^Tz)^2=1+2x^Tz+(x^Tz)^2\\
\because \phi(x)=[1,\sqrt2x_1,\sqrt2x_2,\sqrt2x_1x_2,x_1^2,x_2^2]\\
\therefore\phi(x)^T\phi(x)=1+2x_1z_1+2x_2z_2+2x_1x_2z_1z_2\\+(x_1z_1)^2+(x_2z_2)^2$

因为:

$x^Tz=x_1*z_1+x_2*z_2$

所以:

$k(x,z)=(1+x^Tz)^2=\phi(x)^T\phi(z)$成立

<hr>

**习题3-11**

​	**在软间隔支持向量机中，试给出原始最优化问题的对偶问题，并列出其KKT条件。**

软间隔支持向量机约束最优化问题:

$min\quad \frac{1}{2}||\omega||^2+C\sum_{n=1}^{N}{\xi_n}\\
s.t.\quad 1-y^n(\omega^Tx^n+b)-\xi_n≤0\quad\xi_n≥0$

构造拉格朗日目标函数:

$L(\omega,b,\lambda,\xi_i)=\frac{1}{2}||\omega||^2+C\sum_{n=1}^{N}{\xi_n}+\\\sum_{n=1}^{N}\lambda_n(1-y^n(\omega^Tx^n+b)-\xi_n)$

原始最优化问题等价于:

$min_{\omega,b,\xi_i}max_{\lambda_i≥0}L(\omega,b,\lambda,\xi_i)$

由于拉格朗日函数具有对偶性:

$max_{\lambda_i≥0}min_{\omega,b,\xi_i}L(\omega,b,\lambda,\xi_i)$

KKT条件如下:

$\begin{cases}\lambda_i≥0\\
y_i(\omega_i*x_i+b)-1-\xi_i≥0\\
\lambda_i(y_i(\omega_i*x_i+b)-1-\xi_i)=0
\end{cases}$

