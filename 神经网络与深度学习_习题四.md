# 习题四

**习题4-1**

**对于一个神经元σ(ω<sup>T</sup>x+b)，并使用梯度下降优化参数ω时，如果输入x恒大于0，其收敛速度会比零均值化的输入更慢。**

证：

​	零均值化的输入，使得神经元在0附近，sigmod函数在0点处的导数最大，收敛速度最快，当输入是非均值化时，且x恒大于0 时，ω会沿着同一方向变化(要么恒正，要么恒负)，所以该神经元的ω都会往同一个方向更新，就会造成"Z型更新"现象。

![12](C:\Users\lenovo\Desktop\study\picture\4-1.png)

Eg：以只有两维的w1和w2做例子，因为w1和w2的梯度符号一致，要么都为正要么都为负，都为正时则往第一象限变化，都为负时则往第三象限变化，而如果此时最优解在第四象限，则实际训练路径会远远长于最短路径，就会使得网络收敛速度变慢。

<hr>

**习题4-2**

**试设计一个前馈神经网络来解决XOR问题，要求该前馈神经网络具有两个隐藏神经元和一个输出神经元，并使用ReLU作为激活函数。**

解：如图所示：

![12](C:\Users\lenovo\Desktop\study\picture\4-2.png)

$\omega_1=\left[
\matrix{
  1 & -1\\
  -1 & 1\\
}
\right]\\
b_1=\left[
\matrix{
  0\\
  0
}
\right]\\
\omega_2=\left[
\matrix{
  1\\
  1\\
}
\right]\\
b_2=\left[
\matrix{
  0\\
  0
}
\right]$

使得满足XOR条件：

|  x1  |  x2  |  y   |
| :--: | :--: | :--: |
|  0   |  0   |  0   |
|  0   |  1   |  1   |
|  1   |  0   |  1   |
|  1   |  1   |  0   |

<hr>

**习题4-3**

**试举例说明“死亡ReLU问题”，并提出解决方法。**

解：

​	“死亡ReLU问题”：当训练数据都为负值时(或在反向传播过程中，由于学习率比较大，一个很大的梯度经过ReLU神经元，可能会导致ReLU神经元更新后的是负数)，ReLU(x)=0,进而导致梯度为0，后续参数始终得不到更新，**从而使得ReLU神经元的输入始终是负数，输出始终为0。**

解决方法：使用LeakyReLU函数、PReLU函数、ELU函数或softplus函数。

<hr>

**习题4-4**

**计算Swish函数和GELU函数的导数。**

解：

$\frac{dSwish(x)}{dx}=\frac{dx\sigma(\beta x)}{dx}=\frac{d\frac{x}{1+e^{-\beta x}}}{dx}\\=\frac{e^{-\beta x}(1+\beta x)+1}{(1+e^{-\beta x})^2}$

$\frac{dGeLU(x)}{dx}=\frac{dxP(x≤X)}{dx}=\frac{xdP(x≤X)}{dx}+P(x≤X)\\
=\frac{(x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sqrt(2\pi)\sigma}+\phi(x)$

<hr>

**习题4-5**

**如果限制一个神经网络的总神经元数量(不考虑输入层)为N+1，输出层大小为M<sub>0</sub>，输出层大小为1，隐藏层数为L，每个隐藏层的神经元数量为N/L，试分析参数数量和隐藏层数L之间的关系。**

解：

$num(param)=M_0*\frac{N}{L}+(L-1)(\frac{N}{L})^2+\frac{N}{L}+N+1\\
M_0*\frac{N}{L}：表示从输入到第一层的参数数量\\
(L-1)(\frac{N}{L})^2：表示第一层到第L-1层的参数数量\\
\frac{N}{L}：表示第L层的参数数量\\
N+1：每个神经元的偏置b$

注意：该神经网络采用的是全连接的方式。

<hr>

**习题4-6**

**证明通用近似定理对于具有线性输出层和至少一个使用ReLU激活函数的隐藏层组成的前馈神经网络，也都是适用的。**

证明过程略。(好吧，是因为太难了)

还是进一步理解通用近似定理吧。

​	通用近似定理（Universal Approximation Theorem)告诉我们：**一个仅有单隐藏层的神经网络。在神经元个数足够多的情况下，通过非线性的激活函数，足以拟合任意函数。**这使得我们在思考神经网络的问题的时候，不需要考虑：我的函数是否能够用神经网络拟合，因为它永远可以做到——只需要考虑如何用神经网络做到更好的拟合。

<hr>

**习题4-7**

**为什么在神经网络模型的结构化分析函数中不对偏置b进行正则化？**

答：

​	因为正则化的作用主要是防止过拟合，神经网络模型的参数ω的变化对输入的细微变化会得到不同的输出结果，而对于参数b而言，对于任意输入产生相同的效果，或者可以说对输入的改变是不敏感的，不管输入怎么改变，参数b的贡献就只是加个偏置而已。

​	模型对于输入的微小改变产生了输出的较大差异，这是因为模型的“曲率”太大，而模型的曲率是由ω 决定的， b 不贡献曲率（对输入进行求导， b 是直接约掉的）。

​	所以，偏置b进行正则化对防止过拟合无用。

<hr>

**习题4-8**

**为什么用在用反向传播算法进行参数学习的时候要采用随机参数初始化的方式而不是直接令ω=0，b=0？**

答：

​	因为如果参数都为0，在第一遍前向计算每一层的激活值(输出)都相同，接着在反向传播计算每一层的误差项时，所有权重的更新也相同，这样导致隐藏层神经元没有区分性。出现“**对称权重现象**"。

<hr>

**习题4-9**

**梯度消失问题是否可以通过增加学习率来缓解？**

答：

​	不可以，梯度消失问题指的是当网络层数很深的时候，由于使用的**激活函数的导数小于1**，累乘时越来越小，**梯度不断衰减**，从而消失。改变学习率只能改变学习的快慢，并未改变激活函数。

​	另外，如果学习率过大，与最开始的较大的导数相乘结果非常巨大，可能导致梯度爆炸。

